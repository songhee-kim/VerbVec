{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "### run after AMT_tidyresults\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "%pprint off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_col(dataframe, col_to_move, reference_col, right=True):\n",
    "    col_list = dataframe.columns.values.tolist()\n",
    "    col_list2 = [x for x in col_list if x != col_to_move]\n",
    "    reference_idx = col_list2.index(reference_col)\n",
    "    if right==True:\n",
    "        col_list3 = []\n",
    "        for y in col_list2:\n",
    "            col_list3.append(y)\n",
    "            if y == reference_col:\n",
    "                col_list3.append(col_to_move)\n",
    "        return dataframe[col_list3]\n",
    "    else:\n",
    "        col_list3 = []\n",
    "        for y in col_list2:\n",
    "            col_list3.append(y)\n",
    "            if y == reference_col:\n",
    "                col_list3.insert(-2, col_to_move)\n",
    "        return  dataframe[col_list3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = '/Users/songkim/GoogleDrive/Primary/Projects/VerbVector/AMT/html/experiments/ratings2/results'\n",
    "fpath = '/Users/songkim/GoogleDrive/Primary/Projects/VerbVector/AMT/html/experiments/ratings2/results/results_catchreject_0813.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before catch-based rejection: (4335, 21)\n",
      "after catch-based rejection: (4302, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>stim_id</th>\n",
       "      <th>stim</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>response</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration(s)</th>\n",
       "      <th>RT</th>\n",
       "      <th>buttonpress</th>\n",
       "      <th>turkcode</th>\n",
       "      <th>subject_num</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>occupation</th>\n",
       "      <th>results_id</th>\n",
       "      <th>catch_subject_id</th>\n",
       "      <th>catch_response</th>\n",
       "      <th>incorrectN</th>\n",
       "      <th>badsubject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>159</td>\n",
       "      <td>laugh</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0, 3, 0, 0, 1, 5, 6, 2, 3, 3, 6, 2, 0, ...</td>\n",
       "      <td>2021-05-25 16:26:41</td>\n",
       "      <td>2021-05-25 16:35:15</td>\n",
       "      <td>514</td>\n",
       "      <td>[10451, 2940, 2319, 3999, 2779, 3110, 6525, 75...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, ...</td>\n",
       "      <td>8545510</td>\n",
       "      <td>A3I9XLIHPPWPN1</td>\n",
       "      <td>f</td>\n",
       "      <td>57</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Dataanalystadministrationassistant</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 6, 0, 6, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>295</td>\n",
       "      <td>see</td>\n",
       "      <td>2</td>\n",
       "      <td>[6, 3, 0, 1, 1, 2, 0, 4, 0, 1, 0, 3, 3, 0, 0, ...</td>\n",
       "      <td>2021-05-25 16:29:38</td>\n",
       "      <td>2021-05-25 16:36:24</td>\n",
       "      <td>406</td>\n",
       "      <td>[4024, 9148, 2726, 5130, 5199, 3487, 2790, 432...</td>\n",
       "      <td>[1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>2727964</td>\n",
       "      <td>A2CWJRAEFZ44HU</td>\n",
       "      <td>m</td>\n",
       "      <td>36</td>\n",
       "      <td>14.0</td>\n",
       "      <td>nursingassistantihelpoutpatientsandstaffinahos...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 6, 0, 6, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>214</td>\n",
       "      <td>flatten</td>\n",
       "      <td>3</td>\n",
       "      <td>[3, 0, 0, 0, 5, 3, 1, 1, 3, 2, 0, 5, 0, 1, 4, ...</td>\n",
       "      <td>2021-05-25 16:26:58</td>\n",
       "      <td>2021-05-25 16:36:47</td>\n",
       "      <td>589</td>\n",
       "      <td>[4432, 2912, 2592, 3640, 7976, 6904, 5368, 608...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>3205581</td>\n",
       "      <td>A7ERZELTAMWL5</td>\n",
       "      <td>m</td>\n",
       "      <td>70</td>\n",
       "      <td>18.0</td>\n",
       "      <td>PatrolofficerIridearoundaresidentialcomplextoc...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 6, 0, 6, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>195</td>\n",
       "      <td>increase</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 3, 6, 4, 3, 2, 3, 3, 6, 5, 5, 5, 6, 6, 5, ...</td>\n",
       "      <td>2021-05-25 16:30:25</td>\n",
       "      <td>2021-05-25 16:37:35</td>\n",
       "      <td>430</td>\n",
       "      <td>[2941, 3771, 4392, 2726, 3770, 3690, 2473, 130...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>6434648</td>\n",
       "      <td>A1MKYMYY34DZIO</td>\n",
       "      <td>m</td>\n",
       "      <td>30</td>\n",
       "      <td>16.0</td>\n",
       "      <td>student</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 6, 5, 6, 6]</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>calm</td>\n",
       "      <td>5</td>\n",
       "      <td>[4, 0, 0, 0, 3, 0, 0, 1, 4, 0, 5, 0, 4, 2, 2, ...</td>\n",
       "      <td>2021-05-25 16:28:07</td>\n",
       "      <td>2021-05-25 16:38:09</td>\n",
       "      <td>602</td>\n",
       "      <td>[4142, 2349, 7257, 8933, 4802, 3372, 3349, 830...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, ...</td>\n",
       "      <td>1247441</td>\n",
       "      <td>A2LF84L3K71GR2</td>\n",
       "      <td>f</td>\n",
       "      <td>38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>RestaurantSupervisor</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0, 6, 0, 6, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  stim_id      stim  subject_id  \\\n",
       "0   1      159     laugh           1   \n",
       "1   2      295       see           2   \n",
       "2   3      214   flatten           3   \n",
       "3   4      195  increase           4   \n",
       "4   5      262      calm           5   \n",
       "\n",
       "                                            response                start  \\\n",
       "0  [1, 0, 0, 3, 0, 0, 1, 5, 6, 2, 3, 3, 6, 2, 0, ...  2021-05-25 16:26:41   \n",
       "1  [6, 3, 0, 1, 1, 2, 0, 4, 0, 1, 0, 3, 3, 0, 0, ...  2021-05-25 16:29:38   \n",
       "2  [3, 0, 0, 0, 5, 3, 1, 1, 3, 2, 0, 5, 0, 1, 4, ...  2021-05-25 16:26:58   \n",
       "3  [4, 3, 6, 4, 3, 2, 3, 3, 6, 5, 5, 5, 6, 6, 5, ...  2021-05-25 16:30:25   \n",
       "4  [4, 0, 0, 0, 3, 0, 0, 1, 4, 0, 5, 0, 4, 2, 2, ...  2021-05-25 16:28:07   \n",
       "\n",
       "                   end  duration(s)  \\\n",
       "0  2021-05-25 16:35:15          514   \n",
       "1  2021-05-25 16:36:24          406   \n",
       "2  2021-05-25 16:36:47          589   \n",
       "3  2021-05-25 16:37:35          430   \n",
       "4  2021-05-25 16:38:09          602   \n",
       "\n",
       "                                                  RT  \\\n",
       "0  [10451, 2940, 2319, 3999, 2779, 3110, 6525, 75...   \n",
       "1  [4024, 9148, 2726, 5130, 5199, 3487, 2790, 432...   \n",
       "2  [4432, 2912, 2592, 3640, 7976, 6904, 5368, 608...   \n",
       "3  [2941, 3771, 4392, 2726, 3770, 3690, 2473, 130...   \n",
       "4  [4142, 2349, 7257, 8933, 4802, 3372, 3349, 830...   \n",
       "\n",
       "                                         buttonpress  turkcode  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, ...   8545510   \n",
       "1  [1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   2727964   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   3205581   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   6434648   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, ...   1247441   \n",
       "\n",
       "      subject_num gender  age education  \\\n",
       "0  A3I9XLIHPPWPN1      f   57      18.0   \n",
       "1  A2CWJRAEFZ44HU      m   36      14.0   \n",
       "2   A7ERZELTAMWL5      m   70      18.0   \n",
       "3  A1MKYMYY34DZIO      m   30      16.0   \n",
       "4  A2LF84L3K71GR2      f   38      12.0   \n",
       "\n",
       "                                          occupation  results_id  \\\n",
       "0                 Dataanalystadministrationassistant           1   \n",
       "1  nursingassistantihelpoutpatientsandstaffinahos...           2   \n",
       "2  PatrolofficerIridearoundaresidentialcomplextoc...           3   \n",
       "3                                            student           4   \n",
       "4                               RestaurantSupervisor           5   \n",
       "\n",
       "   catch_subject_id   catch_response  incorrectN badsubject  \n",
       "0                 1  [0, 6, 0, 6, 6]           0          N  \n",
       "1                 2  [0, 6, 0, 6, 6]           0          N  \n",
       "2                 3  [0, 6, 0, 6, 6]           0          N  \n",
       "3                 4  [4, 6, 5, 6, 6]           2          N  \n",
       "4                 5  [0, 6, 0, 6, 6]           0          N  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PART1: add duration \n",
    "from datetime import datetime\n",
    "\n",
    "res = pd.read_csv(fpath, converters={'response': eval, 'RT': eval, 'buttonpress': eval, 'catch_response': eval})\n",
    "\n",
    "start = res['start'].values\n",
    "end = res['end'].values\n",
    "start_obj = [datetime.strptime(s, '%Y-%m-%d %H:%M:%S') for s in start]\n",
    "end_obj = [datetime.strptime(e, '%Y-%m-%d %H:%M:%S') for e in end]\n",
    "duration= [(y - x).seconds for x, y in zip(start_obj, end_obj)]\n",
    "res['duration(s)'] = duration\n",
    "res = rearrange_col(res, 'duration(s)', 'end')\n",
    "#res.tail()\n",
    "\n",
    "######################## check number of responses per lemma\n",
    "# for i in set(res['stim_id']): \n",
    "#     df = res.loc[res['stim_id']==i]\n",
    "#     print (i, df.iloc[1,].stim, df.shape[0])\n",
    "\n",
    "\n",
    "#### PART2: CALCULATE SUBJECT-TO-GROUP CORRELATION PER HIT\n",
    "################## convert N/A to 0 \n",
    "res['catch_response_upd'] = res['catch_response'].apply(lambda x: [int(0) if i=='7' else int(i) for i in x])\n",
    "res['response_upd'] = res['response'].apply(lambda x: [int(0) if i=='7' else int(i) for i in x])\n",
    "\n",
    "res = rearrange_col(res, 'catch_response_upd', 'catch_response')\n",
    "res = rearrange_col(res, 'response_upd', 'response')\n",
    "res\n",
    "\n",
    "res.drop(columns=['catch_response', 'response'], inplace=True)\n",
    "res.rename(columns={'response_upd': 'response', 'catch_response_upd':'catch_response'}, inplace=True)\n",
    "\n",
    "######################### drop bad subjects\n",
    "print('before catch-based rejection:', res.shape)\n",
    "res = res.loc[res['badsubject']=='N', ]\n",
    "res = res.reset_index(drop=True)\n",
    "print ('after catch-based rejection:', res.shape)\n",
    "\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of questions: 72\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stim</th>\n",
       "      <th>subject_ids</th>\n",
       "      <th>NofResponse</th>\n",
       "      <th>avg_ratings</th>\n",
       "      <th>std_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>live</td>\n",
       "      <td>[206, 213, 223, 286, 266, 179, 5, 214, 97, 123...</td>\n",
       "      <td>15</td>\n",
       "      <td>[2.333, 0.933, 0.2, 0.533, 0.533, 0.933, 0.6, ...</td>\n",
       "      <td>[2.024, 1.223, 0.414, 0.99, 0.915, 1.792, 0.98...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>speak</td>\n",
       "      <td>[7, 435, 7, 19, 40, 374, 3, 531, 160, 341, 100...</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.857, 0.143, 0.071, 0.071, 0.429, 0.5, 1.071...</td>\n",
       "      <td>[1.351, 0.363, 0.267, 0.267, 0.852, 0.855, 1.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>listen</td>\n",
       "      <td>[203, 122, 345, 105, 136, 684, 182, 114, 595, ...</td>\n",
       "      <td>13</td>\n",
       "      <td>[1.769, 0.692, 1.0, 0.538, 0.769, 1.154, 1.0, ...</td>\n",
       "      <td>[1.922, 1.797, 2.236, 1.127, 1.787, 1.864, 1.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>walk</td>\n",
       "      <td>[6, 56, 423, 311, 523, 564, 258, 561, 114, 106...</td>\n",
       "      <td>13</td>\n",
       "      <td>[1.077, 0.462, 0.0, 0.077, 0.538, 0.462, 0.154...</td>\n",
       "      <td>[1.754, 0.967, 0.0, 0.277, 1.198, 0.877, 0.376...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>fly</td>\n",
       "      <td>[269, 128, 369, 44, 10, 3, 201, 393, 235, 11, ...</td>\n",
       "      <td>13</td>\n",
       "      <td>[2.769, 1.308, 1.385, 0.923, 1.308, 1.462, 2.5...</td>\n",
       "      <td>[2.351, 2.057, 2.063, 2.253, 2.175, 1.808, 1.8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     stim                                        subject_ids  NofResponse  \\\n",
       "1    live  [206, 213, 223, 286, 266, 179, 5, 214, 97, 123...           15   \n",
       "2   speak  [7, 435, 7, 19, 40, 374, 3, 531, 160, 341, 100...           14   \n",
       "3  listen  [203, 122, 345, 105, 136, 684, 182, 114, 595, ...           13   \n",
       "4    walk  [6, 56, 423, 311, 523, 564, 258, 561, 114, 106...           13   \n",
       "5     fly  [269, 128, 369, 44, 10, 3, 201, 393, 235, 11, ...           13   \n",
       "\n",
       "                                         avg_ratings  \\\n",
       "1  [2.333, 0.933, 0.2, 0.533, 0.533, 0.933, 0.6, ...   \n",
       "2  [0.857, 0.143, 0.071, 0.071, 0.429, 0.5, 1.071...   \n",
       "3  [1.769, 0.692, 1.0, 0.538, 0.769, 1.154, 1.0, ...   \n",
       "4  [1.077, 0.462, 0.0, 0.077, 0.538, 0.462, 0.154...   \n",
       "5  [2.769, 1.308, 1.385, 0.923, 1.308, 1.462, 2.5...   \n",
       "\n",
       "                                         std_ratings  \n",
       "1  [2.024, 1.223, 0.414, 0.99, 0.915, 1.792, 0.98...  \n",
       "2  [1.351, 0.363, 0.267, 0.267, 0.852, 0.855, 1.3...  \n",
       "3  [1.922, 1.797, 2.236, 1.127, 1.787, 1.864, 1.7...  \n",
       "4  [1.754, 0.967, 0.0, 0.277, 1.198, 0.877, 0.376...  \n",
       "5  [2.351, 2.057, 2.063, 2.253, 2.175, 1.808, 1.8...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################### create group average dict ###################### \n",
    "stim_id_n = len(set(res['stim_id']))  ###i.e., 320\n",
    "response_n = len(res.iloc[0][\"response\"])\n",
    "print('# of questions:', response_n)\n",
    "\n",
    "column = []\n",
    "for i in range(1, response_n+1):\n",
    "    column.append('q'+str(i))  ##### column = ['q1', 'q2', ... 'q72']\n",
    "\n",
    "group_dict = {}\n",
    "for i in range(1, stim_id_n+1): #loop over 320 verbs \n",
    "    df = res.loc[res['stim_id']==i, ]\n",
    "    df_responses = pd.DataFrame(df.response.tolist(), columns=column).astype(int)\n",
    "    avg = list(df_responses.mean()) #mean rating for each attribute\n",
    "    avg = [round(a,3) for a in avg]\n",
    "    std = list(df_responses.std())\n",
    "    std = [round(s,3) for s in std] #stdev of rating for each attribute\n",
    "\n",
    "\n",
    "    group_dict[i] = {'stim': df.iloc[0]['stim'], 'subject_ids': df['subject_id'].values.tolist(), \n",
    "                  'NofResponse': df.shape[0], 'avg_ratings': avg, 'std_ratings':std} #avg_rating= mean for each attribute\n",
    "\n",
    "group_df = pd.DataFrame.from_dict(group_dict, orient='index')\n",
    "group_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows before dropping: 4302\n",
      "# of rows after dropping: 4301\n"
     ]
    }
   ],
   "source": [
    "########calculate intersubject correlation and update res\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "#update res so as to include \"subject-to-group\" correlation\n",
    "res_dict = res.to_dict(orient='index')\n",
    "res_dict[1]\n",
    "\n",
    "for k, v in res_dict.items():\n",
    "    response = v['response']\n",
    "    check = v['stim'] == group_dict[v['stim_id']]['stim']\n",
    "    if check == False:\n",
    "        print ('something wrong!')\n",
    "    group_avg = group_dict[v['stim_id']]['avg_ratings']\n",
    "    v['corr'] = round(pearsonr(response, group_avg)[0],3) ###correlation b/w this vector and group average vector for word X.\n",
    "    v['corr_fisher'] = np.arctanh(v['corr'])\n",
    "\n",
    "\n",
    "#pearsonr(response, group_response)[0]\n",
    "\n",
    "### convert res_dict to df\n",
    "res_upd = pd.DataFrame.from_dict(res_dict, orient='index') #res updated\n",
    "\n",
    "### find HITs where the response is constant (i.e., subject hitting the same button all the time)\n",
    "res_upd[res_upd['corr'].isna()]\n",
    "print('# of rows before dropping:', res_upd.shape[0])\n",
    "res_upd = res_upd.dropna(subset=['corr'])\n",
    "print('# of rows after dropping:', res_upd.shape[0])\n",
    "\n",
    "## write res_upd to csv \n",
    "res_upd.to_csv(savepath + '/HITs_with_correlations_0813.csv', index=False) ### HITs after dropping catch-failed HITs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of responses below r= 0.4 : 406\n",
      "before: (4301, 23)\n",
      "after: (3895, 23)\n"
     ]
    }
   ],
   "source": [
    "###### plot histogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# res_upd.hist(column='corr', cumulative=True, bins=150, ax=ax)\n",
    "# fig.savefig(savepath+'/subj_to_group_corr_hist_0813.png') #based on all catch-pass trials\n",
    "\n",
    "###### plot cumulative distribution of corr (\"sort observation by corr\")\n",
    "corr = sorted(res_upd['corr'].tolist())\n",
    "#plt.step(x=corr, y=np.arange(len(corr)))\n",
    "plt.step(x=np.arange(len(corr)), y=corr, c=\"black\")\n",
    "plt.title(\"observations by correlation\")\n",
    "plt.xlabel(\"observation\")\n",
    "plt.ylabel(\"r\")\n",
    "plt.yticks(np.arange(-0.2, 1, 0.1))\n",
    "plt.grid(axis=\"y\")\n",
    "plt.savefig(savepath+'/observations_by_correlation_0813.png')\n",
    "\n",
    "\n",
    "#### drop responses where r < threshold\n",
    "thre = 0.4\n",
    "below_thre = res_upd[res_upd['corr']< thre].shape\n",
    "print('# of responses below r=', thre, ':', below_thre[0])\n",
    "dropind = res_upd[res_upd['corr'] < thre].index.tolist()\n",
    "#res_upd.drop([0,1])\n",
    "print('before:', res_upd.shape)\n",
    "res_upd.drop(dropind, inplace=True)\n",
    "print('after:', res_upd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** From here, data is 'clean', i.e., w.r.t catch trials & sub-to-group correlation*******\n"
     ]
    }
   ],
   "source": [
    "print (\"**** From here, data is 'clean', i.e., w.r.t catch trials & sub-to-group correlation*******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Part 2: find out bad words\n",
    "quesf = '/Users/songkim/GoogleDrive/Primary/Projects/VerbVector/AMT/html/experiments/ratings2/stimuli/questions.txt'\n",
    "ques = pd.read_csv(quesf, delimiter='\\t', header=None).iloc[:, :2].rename(columns={0:'fname', 1:'question'})\n",
    "q_dict = ques.to_dict(orient='index')\n",
    "\n",
    "#find only \"verb\" features\n",
    "new_q = ques.iloc[67:,]\n",
    "caused = ques.loc[ques['fname']=='Caused',]\n",
    "\n",
    "verb_ques = pd.concat([caused, new_q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>Caused</td>\n",
       "      <td>To what extent does this verb describe an even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>Boundedness</td>\n",
       "      <td>Some verbs refer to an activity that could con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>Actor</td>\n",
       "      <td>To what extent does this verb describe somethi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>Done to Something Else</td>\n",
       "      <td>To what extent does this verb describe an acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>State of Being</td>\n",
       "      <td>To what extent does this verb describe a &lt;i&gt;ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>Require Energy Input</td>\n",
       "      <td>To what extent is physical or mental energy re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     fname                                           question\n",
       "46                  Caused  To what extent does this verb describe an even...\n",
       "67             Boundedness  Some verbs refer to an activity that could con...\n",
       "68                   Actor  To what extent does this verb describe somethi...\n",
       "69  Done to Something Else  To what extent does this verb describe an acti...\n",
       "70          State of Being  To what extent does this verb describe a <i>ch...\n",
       "71    Require Energy Input  To what extent is physical or mental energy re..."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>stim_id</th>\n",
       "      <th>stim</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>response</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration(s)</th>\n",
       "      <th>RT</th>\n",
       "      <th>buttonpress</th>\n",
       "      <th>turkcode</th>\n",
       "      <th>subject_num</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>occupation</th>\n",
       "      <th>results_id</th>\n",
       "      <th>catch_subject_id</th>\n",
       "      <th>catch_response</th>\n",
       "      <th>incorrectN</th>\n",
       "      <th>badsubject</th>\n",
       "      <th>corr</th>\n",
       "      <th>corr_fisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>159</td>\n",
       "      <td>laugh</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0, 3, 0, 0, 1, 5, 6, 2, 3, 3, 6, 2, 0, ...</td>\n",
       "      <td>2021-05-25 16:26:41</td>\n",
       "      <td>2021-05-25 16:35:15</td>\n",
       "      <td>514</td>\n",
       "      <td>[10451, 2940, 2319, 3999, 2779, 3110, 6525, 75...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, ...</td>\n",
       "      <td>8545510</td>\n",
       "      <td>A3I9XLIHPPWPN1</td>\n",
       "      <td>f</td>\n",
       "      <td>57</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Dataanalystadministrationassistant</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 6, 0, 6, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.886</td>\n",
       "      <td>1.403008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>295</td>\n",
       "      <td>see</td>\n",
       "      <td>2</td>\n",
       "      <td>[6, 3, 0, 1, 1, 2, 0, 4, 0, 1, 0, 3, 3, 0, 0, ...</td>\n",
       "      <td>2021-05-25 16:29:38</td>\n",
       "      <td>2021-05-25 16:36:24</td>\n",
       "      <td>406</td>\n",
       "      <td>[4024, 9148, 2726, 5130, 5199, 3487, 2790, 432...</td>\n",
       "      <td>[1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>2727964</td>\n",
       "      <td>A2CWJRAEFZ44HU</td>\n",
       "      <td>m</td>\n",
       "      <td>36</td>\n",
       "      <td>14.0</td>\n",
       "      <td>nursingassistantihelpoutpatientsandstaffinahos...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 6, 0, 6, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.991497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>214</td>\n",
       "      <td>flatten</td>\n",
       "      <td>3</td>\n",
       "      <td>[3, 0, 0, 0, 5, 3, 1, 1, 3, 2, 0, 5, 0, 1, 4, ...</td>\n",
       "      <td>2021-05-25 16:26:58</td>\n",
       "      <td>2021-05-25 16:36:47</td>\n",
       "      <td>589</td>\n",
       "      <td>[4432, 2912, 2592, 3640, 7976, 6904, 5368, 608...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>3205581</td>\n",
       "      <td>A7ERZELTAMWL5</td>\n",
       "      <td>m</td>\n",
       "      <td>70</td>\n",
       "      <td>18.0</td>\n",
       "      <td>PatrolofficerIridearoundaresidentialcomplextoc...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 6, 0, 6, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.766</td>\n",
       "      <td>1.010576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>calm</td>\n",
       "      <td>5</td>\n",
       "      <td>[4, 0, 0, 0, 3, 0, 0, 1, 4, 0, 5, 0, 4, 2, 2, ...</td>\n",
       "      <td>2021-05-25 16:28:07</td>\n",
       "      <td>2021-05-25 16:38:09</td>\n",
       "      <td>602</td>\n",
       "      <td>[4142, 2349, 7257, 8933, 4802, 3372, 3349, 830...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, ...</td>\n",
       "      <td>1247441</td>\n",
       "      <td>A2LF84L3K71GR2</td>\n",
       "      <td>f</td>\n",
       "      <td>38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>RestaurantSupervisor</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0, 6, 0, 6, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.795</td>\n",
       "      <td>1.084875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>jump</td>\n",
       "      <td>6</td>\n",
       "      <td>[3, 0, 0, 0, 0, 1, 0, 3, 6, 3, 0, 0, 0, 6, 0, ...</td>\n",
       "      <td>2021-05-25 16:29:58</td>\n",
       "      <td>2021-05-25 16:45:19</td>\n",
       "      <td>921</td>\n",
       "      <td>[9500, 3180, 3371, 90941, 4762, 8544, 6685, 40...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>8286749</td>\n",
       "      <td>A1VSHM4NLZ705D</td>\n",
       "      <td>m</td>\n",
       "      <td>41</td>\n",
       "      <td>12.0</td>\n",
       "      <td>retail</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[0, 6, 0, 6, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.859</td>\n",
       "      <td>1.289517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  stim_id     stim  subject_id  \\\n",
       "0   1      159    laugh           1   \n",
       "1   2      295      see           2   \n",
       "2   3      214  flatten           3   \n",
       "4   5      262     calm           5   \n",
       "5   6        7     jump           6   \n",
       "\n",
       "                                            response                start  \\\n",
       "0  [1, 0, 0, 3, 0, 0, 1, 5, 6, 2, 3, 3, 6, 2, 0, ...  2021-05-25 16:26:41   \n",
       "1  [6, 3, 0, 1, 1, 2, 0, 4, 0, 1, 0, 3, 3, 0, 0, ...  2021-05-25 16:29:38   \n",
       "2  [3, 0, 0, 0, 5, 3, 1, 1, 3, 2, 0, 5, 0, 1, 4, ...  2021-05-25 16:26:58   \n",
       "4  [4, 0, 0, 0, 3, 0, 0, 1, 4, 0, 5, 0, 4, 2, 2, ...  2021-05-25 16:28:07   \n",
       "5  [3, 0, 0, 0, 0, 1, 0, 3, 6, 3, 0, 0, 0, 6, 0, ...  2021-05-25 16:29:58   \n",
       "\n",
       "                   end  duration(s)  \\\n",
       "0  2021-05-25 16:35:15          514   \n",
       "1  2021-05-25 16:36:24          406   \n",
       "2  2021-05-25 16:36:47          589   \n",
       "4  2021-05-25 16:38:09          602   \n",
       "5  2021-05-25 16:45:19          921   \n",
       "\n",
       "                                                  RT  \\\n",
       "0  [10451, 2940, 2319, 3999, 2779, 3110, 6525, 75...   \n",
       "1  [4024, 9148, 2726, 5130, 5199, 3487, 2790, 432...   \n",
       "2  [4432, 2912, 2592, 3640, 7976, 6904, 5368, 608...   \n",
       "4  [4142, 2349, 7257, 8933, 4802, 3372, 3349, 830...   \n",
       "5  [9500, 3180, 3371, 90941, 4762, 8544, 6685, 40...   \n",
       "\n",
       "                                         buttonpress  turkcode  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, ...   8545510   \n",
       "1  [1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   2727964   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   3205581   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, ...   1247441   \n",
       "5  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   8286749   \n",
       "\n",
       "      subject_num gender  age education  \\\n",
       "0  A3I9XLIHPPWPN1      f   57      18.0   \n",
       "1  A2CWJRAEFZ44HU      m   36      14.0   \n",
       "2   A7ERZELTAMWL5      m   70      18.0   \n",
       "4  A2LF84L3K71GR2      f   38      12.0   \n",
       "5  A1VSHM4NLZ705D      m   41      12.0   \n",
       "\n",
       "                                          occupation  results_id  \\\n",
       "0                 Dataanalystadministrationassistant           1   \n",
       "1  nursingassistantihelpoutpatientsandstaffinahos...           2   \n",
       "2  PatrolofficerIridearoundaresidentialcomplextoc...           3   \n",
       "4                               RestaurantSupervisor           5   \n",
       "5                                             retail           6   \n",
       "\n",
       "   catch_subject_id   catch_response  incorrectN badsubject   corr  \\\n",
       "0                 1  [0, 6, 0, 6, 6]           0          N  0.886   \n",
       "1                 2  [0, 6, 0, 6, 6]           0          N  0.758   \n",
       "2                 3  [0, 6, 0, 6, 6]           0          N  0.766   \n",
       "4                 5  [0, 6, 0, 6, 6]           0          N  0.795   \n",
       "5                 6  [0, 6, 0, 6, 6]           0          N  0.859   \n",
       "\n",
       "   corr_fisher  \n",
       "0     1.403008  \n",
       "1     0.991497  \n",
       "2     1.010576  \n",
       "4     1.084875  \n",
       "5     1.289517  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_upd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stim_id</th>\n",
       "      <th>stim</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>response</th>\n",
       "      <th>duration(s)</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>incorrectN</th>\n",
       "      <th>corr_fisher</th>\n",
       "      <th>Vision</th>\n",
       "      <th>Bright</th>\n",
       "      <th>Dark</th>\n",
       "      <th>Color</th>\n",
       "      <th>Pattern</th>\n",
       "      <th>Large</th>\n",
       "      <th>Small</th>\n",
       "      <th>Motion</th>\n",
       "      <th>Biomotion</th>\n",
       "      <th>Fast</th>\n",
       "      <th>Slow</th>\n",
       "      <th>Shape</th>\n",
       "      <th>Face</th>\n",
       "      <th>Body</th>\n",
       "      <th>Touch</th>\n",
       "      <th>Hot</th>\n",
       "      <th>Cold</th>\n",
       "      <th>Smooth</th>\n",
       "      <th>Rough</th>\n",
       "      <th>Light</th>\n",
       "      <th>Heavy</th>\n",
       "      <th>Pain</th>\n",
       "      <th>Audition</th>\n",
       "      <th>Loud</th>\n",
       "      <th>Low</th>\n",
       "      <th>High</th>\n",
       "      <th>Sound</th>\n",
       "      <th>Music</th>\n",
       "      <th>Speech</th>\n",
       "      <th>Taste</th>\n",
       "      <th>Smell</th>\n",
       "      <th>Head</th>\n",
       "      <th>UpperLimb</th>\n",
       "      <th>LowerLimb</th>\n",
       "      <th>Practice</th>\n",
       "      <th>Landmark</th>\n",
       "      <th>Path</th>\n",
       "      <th>Scene</th>\n",
       "      <th>Near</th>\n",
       "      <th>Toward</th>\n",
       "      <th>Away</th>\n",
       "      <th>Number</th>\n",
       "      <th>Time</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Long</th>\n",
       "      <th>Short</th>\n",
       "      <th>Caused</th>\n",
       "      <th>Consequential</th>\n",
       "      <th>Social</th>\n",
       "      <th>Human</th>\n",
       "      <th>Communication</th>\n",
       "      <th>Self</th>\n",
       "      <th>Cognition</th>\n",
       "      <th>Benefit</th>\n",
       "      <th>Harm</th>\n",
       "      <th>Pleasant</th>\n",
       "      <th>Unpleasant</th>\n",
       "      <th>Happy</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Angry</th>\n",
       "      <th>Disgusted</th>\n",
       "      <th>Fearful</th>\n",
       "      <th>Surprised</th>\n",
       "      <th>Drive</th>\n",
       "      <th>Needs</th>\n",
       "      <th>Attention</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>Boundedness</th>\n",
       "      <th>Actor</th>\n",
       "      <th>Done to Something Else</th>\n",
       "      <th>State of Being</th>\n",
       "      <th>Require Energy Input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>laugh</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0, 3, 0, 0, 1, 5, 6, 2, 3, 3, 6, 2, 0, ...</td>\n",
       "      <td>514</td>\n",
       "      <td>57</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.403008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>295</td>\n",
       "      <td>see</td>\n",
       "      <td>2</td>\n",
       "      <td>[6, 3, 0, 1, 1, 2, 0, 4, 0, 1, 0, 3, 3, 0, 0, ...</td>\n",
       "      <td>406</td>\n",
       "      <td>36</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.991497</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>214</td>\n",
       "      <td>flatten</td>\n",
       "      <td>3</td>\n",
       "      <td>[3, 0, 0, 0, 5, 3, 1, 1, 3, 2, 0, 5, 0, 1, 4, ...</td>\n",
       "      <td>589</td>\n",
       "      <td>70</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.010576</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>262</td>\n",
       "      <td>calm</td>\n",
       "      <td>5</td>\n",
       "      <td>[4, 0, 0, 0, 3, 0, 0, 1, 4, 0, 5, 0, 4, 2, 2, ...</td>\n",
       "      <td>602</td>\n",
       "      <td>38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.084875</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>jump</td>\n",
       "      <td>6</td>\n",
       "      <td>[3, 0, 0, 0, 0, 1, 0, 3, 6, 3, 0, 0, 0, 6, 0, ...</td>\n",
       "      <td>921</td>\n",
       "      <td>41</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.289517</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stim_id     stim  subject_id  \\\n",
       "0      159    laugh           1   \n",
       "1      295      see           2   \n",
       "2      214  flatten           3   \n",
       "4      262     calm           5   \n",
       "5        7     jump           6   \n",
       "\n",
       "                                            response  duration(s)  age  \\\n",
       "0  [1, 0, 0, 3, 0, 0, 1, 5, 6, 2, 3, 3, 6, 2, 0, ...          514   57   \n",
       "1  [6, 3, 0, 1, 1, 2, 0, 4, 0, 1, 0, 3, 3, 0, 0, ...          406   36   \n",
       "2  [3, 0, 0, 0, 5, 3, 1, 1, 3, 2, 0, 5, 0, 1, 4, ...          589   70   \n",
       "4  [4, 0, 0, 0, 3, 0, 0, 1, 4, 0, 5, 0, 4, 2, 2, ...          602   38   \n",
       "5  [3, 0, 0, 0, 0, 1, 0, 3, 6, 3, 0, 0, 0, 6, 0, ...          921   41   \n",
       "\n",
       "  education  incorrectN  corr_fisher  Vision  Bright  Dark  Color  Pattern  \\\n",
       "0      18.0           0     1.403008       1       0     0      3        0   \n",
       "1      14.0           0     0.991497       6       3     0      1        1   \n",
       "2      18.0           0     1.010576       3       0     0      0        5   \n",
       "4      12.0           0     1.084875       4       0     0      0        3   \n",
       "5      12.0           0     1.289517       3       0     0      0        0   \n",
       "\n",
       "   Large  Small  Motion  Biomotion  Fast  Slow  Shape  Face  Body  Touch  Hot  \\\n",
       "0      0      1       5          6     2     3      3     6     2      0    0   \n",
       "1      2      0       4          0     1     0      3     3     0      0    0   \n",
       "2      3      1       1          3     2     0      5     0     1      4    0   \n",
       "4      0      0       1          4     0     5      0     4     2      2    2   \n",
       "5      1      0       3          6     3     0      0     0     6      0    0   \n",
       "\n",
       "   Cold  Smooth  Rough  Light  Heavy  Pain  Audition  Loud  Low  High  Sound  \\\n",
       "0     0       0      0      0      0     0         2     2    2     5      6   \n",
       "1     0       0      0      0      0     0         0     0    0     0      0   \n",
       "2     0       0      0      0      1     0         0     0    0     0      0   \n",
       "4     0       1      0      0      0     0         3     0    1     0      1   \n",
       "5     0       0      0      0      0     1         0     0    0     0      0   \n",
       "\n",
       "   Music  Speech  Taste  Smell  Head  UpperLimb  LowerLimb  Practice  \\\n",
       "0      1       4      0      0     6          0          0         6   \n",
       "1      0       0      0      0     6          0          0         6   \n",
       "2      0       0      0      0     0          4          1         3   \n",
       "4      3       4      0      2     2          0          0         3   \n",
       "5      0       0      0      0     0          0          6         6   \n",
       "\n",
       "   Landmark  Path  Scene  Near  Toward  Away  Number  Time  Duration  Long  \\\n",
       "0         0     0      2     0       0     0       0     0         3     0   \n",
       "1         1     0      0     2       0     0       0     0         0     0   \n",
       "2         0     0      1     2       0     0       0     0         0     4   \n",
       "4         3     0      3     2       6     3       0     3         1     2   \n",
       "5         3     3      3     2       2     0       0     0         6     0   \n",
       "\n",
       "   Short  Caused  Consequential  Social  Human  Communication  Self  \\\n",
       "0      5       2              3       5      4              4     6   \n",
       "1      1       0              0       0      1              4     2   \n",
       "2      1       4              4       2      5              0     0   \n",
       "4      3       4              2       3      5              1     4   \n",
       "5      6       3              2       3      6              0     1   \n",
       "\n",
       "   Cognition  Benefit  Harm  Pleasant  Unpleasant  Happy  Sad  Angry  \\\n",
       "0          5        5     0         6           0      6    0      1   \n",
       "1          0        1     0         4           0      0    0      0   \n",
       "2          0        5     2         4           1      0    0      0   \n",
       "4          6        6     1         6           0      6    0      0   \n",
       "5          0        3     3         1           0      0    0      0   \n",
       "\n",
       "   Disgusted  Fearful  Surprised  Drive  Needs  Attention  Arousal  \\\n",
       "0          0        1          3      3      1          5        6   \n",
       "1          0        0          0      1      2          4        1   \n",
       "2          0        0          0      2      0          1        0   \n",
       "4          0        0          0      6      0          0        0   \n",
       "5          0        2          2      6      1          6        3   \n",
       "\n",
       "   Boundedness  Actor  Done to Something Else  State of Being  \\\n",
       "0            1      4                       0               5   \n",
       "1            0      5                       0               0   \n",
       "2            4      5                       6               0   \n",
       "4            0      3                       0               6   \n",
       "5            1      5                       3               6   \n",
       "\n",
       "   Require Energy Input  \n",
       "0                     4  \n",
       "1                     1  \n",
       "2                     3  \n",
       "4                     3  \n",
       "5                     6  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## export data (for each verb, one response for each feature)\n",
    "res_clean = res_upd[[\"stim_id\", \"stim\", \"subject_id\", \"response\", \"duration(s)\", \"age\", \"education\", \"incorrectN\", \"corr_fisher\"]]\n",
    "new_column= list(range(response_n)) #[0,1,2,...71]\n",
    "res_clean[new_column] = pd.DataFrame(res_clean.response.tolist(), index= res_clean.index)\n",
    "\n",
    "d = dict()\n",
    "#for i in [46,67,68,69,70,71]:\n",
    "for i in range(0,response_n):\n",
    "    d[i] = q_dict[i]['fname']\n",
    "\n",
    "res_clean = res_clean.rename(columns=d)\n",
    "#res_clean = res_clean.drop(columns=['response'])\n",
    "\n",
    "##export res_sub\n",
    "savepath = '/Users/songkim/GoogleDrive/Primary/Projects/VerbVector/AMT/html/experiments/ratings2/results'\n",
    "res_clean.to_csv(savepath+'/clean_HITs_0813.csv', index=False) ##HITs that only include crucial info\n",
    "res_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### create df of standard deviation and mean for each word (i.e., word x features)\n",
    "res_clean = pd.read_csv(savepath+'/clean_HITs_0813.csv')\n",
    "stim_ids = sorted(res_clean['stim_id'].unique())\n",
    "\n",
    "res_clean.head()\n",
    "\n",
    "#create new empty df \n",
    "orig_cols = res_clean.columns.values.tolist()\n",
    "rmlist = ['subject_id', 'response', 'age', 'education', 'duration(s)', 'incorrectN', 'corr_fisher']\n",
    "new_cols = [o for o in orig_cols if o not in rmlist]\n",
    "\n",
    "df_sd = pd.DataFrame(columns=new_cols)\n",
    "\n",
    "#1. iterate over 320 verbs and append new row (SD) to df\n",
    "for i in stim_ids:\n",
    "    df_sel = res_clean.loc[res_clean['stim_id']==i, ]\n",
    "    stim_id = df_sel['stim_id'].unique()[0]\n",
    "    stim = df_sel['stim'].unique()[0]\n",
    "\n",
    "    df_features = df_sel.drop(columns=['stim_id', 'stim', 'subject_id', 'response', 'age', 'education', \n",
    "                                       'duration(s)', 'incorrectN', 'corr_fisher'])\n",
    "    new_row_sd = round(df_features.std(),2).to_frame().T #row of standard deviations\n",
    "    new_row_sd['mean_72feat_SD'] = round(new_row_sd.mean(numeric_only=True, axis=1),3).values[0]\n",
    "    sixfeatures = [\"Caused\", \"Boundedness\", \"Actor\", \"Done to Something Else\", \"State of Being\", \"Require Energy Input\"]\n",
    "    new_row_sd['mean_6feat_SD'] = round(new_row_sd.loc[:, sixfeatures].mean(axis=1),3).values[0]\n",
    "    new_row_sd['stim_id'] = stim_id\n",
    "    new_row_sd['stim'] = stim\n",
    "    new_row_sd['responseN'] = int(df_sel.shape[0])\n",
    "    df_sd = df_sd.append(new_row_sd, sort=False)\n",
    "    del new_row_sd\n",
    "\n",
    "\n",
    "# #2. iterate over 320 verbs and append new row (mean) to df\n",
    "df_mean = pd.DataFrame(columns=new_cols)\n",
    "\n",
    "for k in stim_ids:\n",
    "    df_sel = res_clean.loc[res_clean['stim_id']==k, ]\n",
    "    stim_id = df_sel['stim_id'].unique()[0]\n",
    "    stim = df_sel['stim'].unique()[0]\n",
    "\n",
    "    df_features = df_sel.drop(columns=['stim_id', 'stim', 'subject_id', 'response', 'age', 'education', \n",
    "                                       'duration(s)', 'incorrectN', 'corr_fisher'])\n",
    "    new_row_mean = round(df_features.mean(),2).to_frame().T\n",
    "    new_row_mean['stim_id'] = stim_id\n",
    "    new_row_mean['stim'] = stim\n",
    "    df_mean = df_mean.append(new_row_mean, sort=False)\n",
    "    del new_row_mean\n",
    "    \n",
    "\n",
    "col_mean = [n+'-m' if new_cols.index(n)>1 else n for n in new_cols] # m meaning 'mean'\n",
    "df_mean.columns = col_mean\n",
    "df_mean = df_mean.drop(columns = 'stim')\n",
    "#df_mean.head()\n",
    "\n",
    "#3. add mean SD for each row\n",
    "if df_mean.shape[0] != df_sd.shape[0]:\n",
    "    print ('mean df and sd df have different lengths!')\n",
    "\n",
    "df_fin = pd.merge(df_sd, df_mean, on = 'stim_id') # df that has both SD and MEAN (fin meaning 'final')\n",
    "\n",
    "# clean up\n",
    "df_fin['responseN'] = df_fin['responseN'].astype(int)\n",
    "df_fin = rearrange_col(df_fin, 'responseN', 'stim')\n",
    "df_fin = rearrange_col(df_fin, 'mean_72feat_SD', 'responseN')\n",
    "df_fin = rearrange_col(df_fin, 'mean_6feat_SD', 'mean_72feat_SD')\n",
    "\n",
    "df_fin.head()\n",
    "df_fin.to_csv(savepath + '/clean_320verbs_SD_Mean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########PART3: move to verblist (integrate with rating information)\n",
    "sdpath = savepath + '/clean_320verbs_SD_Mean.csv'\n",
    "spreadsheetpath = '/Users/songkim/GoogleDrive/Primary/Projects/VerbVector/Verblists/List320.csv'\n",
    "\n",
    "df_sd = pd.read_csv(sdpath, index_col=0)\n",
    "df_ss = pd.read_csv(spreadsheetpath) #ss meaning spreadsheet\n",
    "df_ss = df_ss.loc[df_ss['in']==1]\n",
    "check = df_sd.shape[0] == df_ss.shape[0]\n",
    "if check !=1:\n",
    "    print ('spreadsheet and sd table have diff lengths!')\n",
    "\n",
    "### add sd and mean to the verblist\n",
    "df_ss2 = pd.merge(left=df_ss, right=df_sd, left_on= 'Lemma', right_on='stim')\n",
    "\n",
    "### clean up merged df\n",
    "df_ss2.drop(columns=['badverb', 'GrandIndex', 'in', 'whyexcluded', 'stim', 'abs_marginality', \n",
    "                     'rel_marginality', 'SynClassLitrt', 'SynClassSK', 'good'], inplace=True)\n",
    "\n",
    "#### add mean (subject-to-group) correlation to df_ss2 ##### (optional)\n",
    "corr_raw = pd.read_csv(savepath + '/HITs_with_correlations_0813.csv')\n",
    "corr_thre = corr_raw[corr_raw['corr']>=thre] #only keep HITs above threshold (of raw corr)\n",
    "#corr_thre.shape\n",
    "\n",
    "corr_thre = corr_thre[[\"stim_id\", \"stim\", \"subject_id\", \"corr\", \"corr_fisher\"]]\n",
    "corr_thre['corr_fisher'] = corr_thre['corr_fisher'].round(decimals=3)\n",
    "corr_thre['corr'] = corr_thre['corr'].round(decimals=3)   \n",
    "\n",
    "from statistics import mean\n",
    "corr_dict = {}\n",
    "for i in range(1,321):\n",
    "    df_sel = corr_thre.loc[corr_thre['stim_id']==i]\n",
    "    stim_id = df_sel.iloc[0]['stim_id']\n",
    "    stim = df_sel.iloc[0]['stim']\n",
    "    corr_list = df_sel['corr_fisher'].values.tolist()\n",
    "    corr_list.sort(reverse=True)\n",
    "    corr_list = [round(c,3) for c in corr_list]\n",
    "    mean_corr = round(df_sel['corr_fisher'].mean(), 3)\n",
    "    \n",
    "    d_add = {'stim_id': stim_id, 'stim': stim, 'corr_list':corr_list, 'mean_fcorr':mean_corr}\n",
    "    corr_dict[i] = d_add\n",
    "\n",
    "corr_df = pd.DataFrame.from_dict(corr_dict, orient='index')\n",
    "#corr_df.head()\n",
    "# corr_df[corr_df['stim']=='urinate']\n",
    "\n",
    "df_ss_out = pd.merge(left=df_ss2, right=corr_df, left_on='Lemma', right_on='stim')\n",
    "df_ss_out = df_ss_out.drop(columns=['stim'])\n",
    "df_ss_out = rearrange_col(df_ss_out, 'responseN', 'freq_matched')\n",
    "# df_sd_out = rearrange_col(df_sd_out, 'corr_list', 'mean_fcorr')\n",
    "df_ss_out = rearrange_col(df_ss_out, 'mean_72feat_SD', 'responseN')\n",
    "df_ss_out = rearrange_col(df_ss_out, 'mean_6feat_SD', 'mean_72feat_SD')\n",
    "df_ss_out = rearrange_col(df_ss_out, 'mean_fcorr', 'mean_6feat_SD')\n",
    "df_ss_out = rearrange_col(df_ss_out, 'stim_id', 'Lemma')\n",
    "df_ss_out = rearrange_col(df_ss_out, 'corr_list', 'mean_fcorr', right=False)\n",
    "######\n",
    "df_ss_out.head()\n",
    "\n",
    "### write to csv\n",
    "savepath = '/Users/songkim/GoogleDrive/Primary/Projects/VerbVector/AMT/html/experiments/ratings2/results'\n",
    "df_ss_out.to_csv(savepath + '/list320_with_ratingsummary_0813.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== SD for 6 feature =======\n",
      "10 percent: 1.414\n",
      "50 percent: 1.688\n",
      "90 percent: 1.938\n",
      "== SD for all features =====\n",
      "10 percent: 1.055\n",
      "50 percent: 1.25\n",
      "90 percent: 1.439\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df_ss_out.plot(y=[\"mean_fcorr\"], kind=\"hist\")\n",
    "df_ss_out.plot(y=[\"mean_72feat_SD\"], kind=\"hist\")\n",
    "#df_ss_out.plot(y=[\"mean_6feat_SD\"], kind=\"hist\")\n",
    "# df_ss_out.plot(x=\"mean_72feat_SD\", y=\"mean_fcorr\", kind=\"scatter\")\n",
    "\n",
    "# find percentiles\n",
    "print ('== SD for 6 feature =======')\n",
    "for x in [10, 50, 90]:\n",
    "    percentile = np.percentile(df_ss_out['mean_6feat_SD'], x, interpolation='midpoint')\n",
    "    print (x,'percent:', round(percentile,3))\n",
    "    \n",
    "print ('== SD for all features =====')\n",
    "for y in [10, 50, 90]:\n",
    "    percentile = np.percentile(df_ss_out['mean_72feat_SD'], y, interpolation='midpoint')\n",
    "    print (y,'percent:', round(percentile,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### PART 4: WITH UPDATED VERBLIST, NARROW DOWN TO 200 VERBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of durations: 3895\n",
      "# of durations after trimming: 3809\n",
      "['Lemma', 'stim_id', 'syn_matched', 'freq_matched', 'responseN', 'mean_72feat_SD', 'corr_list', 'mean_6feat_SD', 'mean_fcorr', 'LogFreqHAL', 'Concreteness', 'AgeofAcqsn', 'Database', 'Trans_Intrans', 'SynClass', 'Note', 'google_intrans', 'google_trans', 'AspClass', 'ChangeOfState', 'SemClass', 'SemanticTypeLevin', 'SemanticTypeEntry', 'Percentage_dom_PoS', 'Length', 'Ortho_N', 'Phono_N', 'OLD', 'PLD', 'NPhon', 'NSyll', 'I_Mean_RT', 'I_Mean_Accuracy', 'I_NMG_Mean_RT', 'I_NMG_Mean_Accuracy', 'N2_F', 'N3_F', 'I_Mean_RT_z', 'I_Mean_Accuracy_z', 'AgeofAcqsn_z', 'LogFreqHAL_z', 'Vision', 'Bright', 'Dark', 'Color', 'Pattern', 'Large', 'Small', 'Motion', 'Biomotion', 'Fast', 'Slow', 'Shape', 'Face', 'Body', 'Touch', 'Hot', 'Cold', 'Smooth', 'Rough', 'Light', 'Heavy', 'Pain', 'Audition', 'Loud', 'Low', 'High', 'Sound', 'Music', 'Speech', 'Taste', 'Smell', 'Head', 'UpperLimb', 'LowerLimb', 'Practice', 'Landmark', 'Path', 'Scene', 'Near', 'Toward', 'Away', 'Number', 'Time', 'Duration', 'Long', 'Short', 'Caused', 'Consequential', 'Social', 'Human', 'Communication', 'Self', 'Cognition', 'Benefit', 'Harm', 'Pleasant', 'Unpleasant', 'Happy', 'Sad', 'Angry', 'Disgusted', 'Fearful', 'Surprised', 'Drive', 'Needs', 'Attention', 'Arousal', 'Boundedness', 'Actor', 'Done to Something Else', 'State of Being', 'Require Energy Input', 'Vision-m', 'Bright-m', 'Dark-m', 'Color-m', 'Pattern-m', 'Large-m', 'Small-m', 'Motion-m', 'Biomotion-m', 'Fast-m', 'Slow-m', 'Shape-m', 'Face-m', 'Body-m', 'Touch-m', 'Hot-m', 'Cold-m', 'Smooth-m', 'Rough-m', 'Light-m', 'Heavy-m', 'Pain-m', 'Audition-m', 'Loud-m', 'Low-m', 'High-m', 'Sound-m', 'Music-m', 'Speech-m', 'Taste-m', 'Smell-m', 'Head-m', 'UpperLimb-m', 'LowerLimb-m', 'Practice-m', 'Landmark-m', 'Path-m', 'Scene-m', 'Near-m', 'Toward-m', 'Away-m', 'Number-m', 'Time-m', 'Duration-m', 'Long-m', 'Short-m', 'Caused-m', 'Consequential-m', 'Social-m', 'Human-m', 'Communication-m', 'Self-m', 'Cognition-m', 'Benefit-m', 'Harm-m', 'Pleasant-m', 'Unpleasant-m', 'Happy-m', 'Sad-m', 'Angry-m', 'Disgusted-m', 'Fearful-m', 'Surprised-m', 'Drive-m', 'Needs-m', 'Attention-m', 'Arousal-m', 'Boundedness-m', 'Actor-m', 'Done to Something Else-m', 'State of Being-m', 'Require Energy Input-m']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot object at 0x7fe0218656d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### explore rating\n",
    "\n",
    "clean_hit = pd.read_csv(savepath+'/clean_HITs_0813.csv')\n",
    "clean_hit.head()\n",
    "\n",
    "#1) plot duration\n",
    "#clean_hit.plot(y=[\"duration(s)\"], kind=\"hist\", bins=100)\n",
    "dur = sorted(clean_hit['duration(s)'].tolist())\n",
    "print('# of durations:', len(dur))\n",
    "dur_trim = [d for d in dur if d < 3000]\n",
    "print ('# of durations after trimming:', len(dur_trim))\n",
    "# plt.step(x=np.arange(len(dur_trim)), y=dur_trim, c=\"black\")\n",
    "# plt.xlabel(\"observation\")\n",
    "# plt.ylabel(\"duration (in sec)\")\n",
    "\n",
    "\n",
    "## 1) correlation b/w \n",
    "clean_hit = clean_hit[clean_hit['education']<30]\n",
    "clean_hit.plot(x=\"education\", y=\"corr_fisher\", kind=\"scatter\")\n",
    "\n",
    "\n",
    "clean_verbs = pd.read_csv(savepath + '/list320_with_ratingsummary_0813.csv')\n",
    "#clean_verbs.head()\n",
    "print(clean_verbs.columns.tolist())\n",
    "#2) \n",
    "clean_verbs.plot(x=\"Concreteness\", y=\"mean_6feat_SD\", kind=\"scatter\") \n",
    "clean_verbs.plot(x=\"AgeofAcqsn\", y=\"mean_6feat_SD\", kind=\"scatter\") \n",
    "clean_verbs.plot(x=\"LogFreqHAL\", y=\"mean_6feat_SD\", kind=\"scatter\")\n",
    "\n",
    "# years of education & deviation from group mean\n",
    "# time spent & deviation from group mean++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantitative differences between a priori categories\n",
    "syn_cat = ['unerg', 'unacc', 'trans']\n",
    "asp_cat = ['activity', 'state', 'achvm']\n",
    "# For each comparison, an unpaired t-test was conducted for 72 attributes ()\n",
    "# x = ['vision' rating for unergatives], y = ['vision' rating for unaccusative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k means cluster analysis\n",
    "### k-means cluster analysis of the entire 320 word set, with k =? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resume', 'intensify', 'suppress', 'constitute', 'consolidate', 'disperse', 'deteriorate', 'integrate', 'summon', 'condense']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### pick bad verbs \n",
    "n=60\n",
    "df = pd.read_csv(savepath + '/list320_with_ratingsummary_0813.csv')\n",
    "df_sub = df[(df['syn_matched']==0) & (df['freq_matched']==0)]\n",
    "by_72sd = df_sub.sort_values(by='mean_72feat_SD', ascending=False)['Lemma'].tolist()[:n]\n",
    "by_6sd = df_sub.sort_values(by='mean_6feat_SD', ascending=False)['Lemma'].tolist()[:n]\n",
    "by_freq = df_sub.sort_values(by='LogFreqHAL', ascending=True)['Lemma'].tolist()[:n]\n",
    "by_ageofaqsn = df_sub.sort_values(by='AgeofAcqsn', ascending=False)['Lemma'].tolist()[:n]\n",
    "by_meancorr = df_sub.sort_values(by='mean_fcorr', ascending=True)['Lemma'].tolist()[:n]\n",
    "#df.shape[0] - df_sub.shape[0]\n",
    "#df[(df['category'] == 'A') & (df['value'].between(10,20))]\n",
    "by_ageofaqsn[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['constitute', 'subside', 'dampen', 'deepen', 'detect', 'recognize', 'throb', 'diminish', 'stop', 'stiffen', 'break', 'submerge', 'extract', 'insert', 'eliminate', 'disable', 'intensify', 'appear', 'drop', 'vaporize']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_meancorr[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verb</th>\n",
       "      <th>count</th>\n",
       "      <th>AspClass</th>\n",
       "      <th>Database</th>\n",
       "      <th>Semclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>purify</td>\n",
       "      <td>5</td>\n",
       "      <td>Process</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>ch of state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>endure</td>\n",
       "      <td>5</td>\n",
       "      <td>Process</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>cognitive event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>dampen</td>\n",
       "      <td>4</td>\n",
       "      <td>Process</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>ch of state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>accelerate</td>\n",
       "      <td>4</td>\n",
       "      <td>Process</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>ch of state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>involve</td>\n",
       "      <td>4</td>\n",
       "      <td>State</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>intensify</td>\n",
       "      <td>4</td>\n",
       "      <td>Process</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>ch of quantity or size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>extract</td>\n",
       "      <td>4</td>\n",
       "      <td>Process</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>obj-directed action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>detect</td>\n",
       "      <td>4</td>\n",
       "      <td>Achievement</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>perception</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>isolate</td>\n",
       "      <td>4</td>\n",
       "      <td>Process</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>vaporize</td>\n",
       "      <td>4</td>\n",
       "      <td>Achievement</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>ch of state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>diminish</td>\n",
       "      <td>4</td>\n",
       "      <td>Process</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>emission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>solidify</td>\n",
       "      <td>4</td>\n",
       "      <td>Process</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>ch of state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>constitute</td>\n",
       "      <td>4</td>\n",
       "      <td>State</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>deteriorate</td>\n",
       "      <td>4</td>\n",
       "      <td>Process</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>ch of state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>omit</td>\n",
       "      <td>4</td>\n",
       "      <td>Achievement</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>generate</td>\n",
       "      <td>4</td>\n",
       "      <td>Process</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>creation or destruction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>classify</td>\n",
       "      <td>4</td>\n",
       "      <td>Achievement</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>cognitive event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>deepen</td>\n",
       "      <td>4</td>\n",
       "      <td>Process</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>ch of quantity or size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>summon</td>\n",
       "      <td>4</td>\n",
       "      <td>Achievement</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>social or interpersonal event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>sharpen</td>\n",
       "      <td>4</td>\n",
       "      <td>Process</td>\n",
       "      <td>VerbNet</td>\n",
       "      <td>obj-directed action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            verb count     AspClass Database                       Semclass\n",
       "101       purify     5      Process  VerbNet                    ch of state\n",
       "42        endure     5      Process  VerbNet                cognitive event\n",
       "122       dampen     4      Process  VerbNet                    ch of state\n",
       "46    accelerate     4      Process  VerbNet                    ch of state\n",
       "44       involve     4        State  VerbNet                          other\n",
       "41     intensify     4      Process  VerbNet         ch of quantity or size\n",
       "40       extract     4      Process  VerbNet            obj-directed action\n",
       "39        detect     4  Achievement  VerbNet                     perception\n",
       "86       isolate     4      Process  VerbNet                          other\n",
       "103     vaporize     4  Achievement  VerbNet                    ch of state\n",
       "33      diminish     4      Process  VerbNet                       emission\n",
       "83      solidify     4      Process  VerbNet                    ch of state\n",
       "24    constitute     4        State  VerbNet                          other\n",
       "116  deteriorate     4      Process  VerbNet                    ch of state\n",
       "21          omit     4  Achievement  VerbNet                          other\n",
       "65      generate     4      Process  VerbNet        creation or destruction\n",
       "9       classify     4  Achievement  VerbNet                cognitive event\n",
       "5         deepen     4      Process  VerbNet         ch of quantity or size\n",
       "13        summon     4  Achievement  VerbNet  social or interpersonal event\n",
       "127      sharpen     4      Process  VerbNet            obj-directed action"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = by_72sd + by_6sd + by_freq + by_ageofaqsn + by_meancorr\n",
    "df_lookup = clean_verbs.loc[clean_verbs['Lemma'].isin(combined)]\n",
    "verbs = list(set(combined))\n",
    "bad_df = pd.DataFrame(columns = ['verb', 'count'])\n",
    "for n in range(len(verbs)):\n",
    "    ind = df_lookup.index[df_lookup['Lemma']==verbs[n]].values[0]\n",
    "    bad_df = bad_df.append({'verb': verbs[n], 'count': combined.count(verbs[n]),\n",
    "                            'Database': df_lookup.at[ind, 'Database'],\n",
    "                           'AspClass': df_lookup.at[ind, 'AspClass'],\n",
    "                           'Semclass': df_lookup.at[ind, 'SemClass']},\n",
    "                           ignore_index=True) \n",
    "    #print (n+1, verbs[n], combined.count(verbs[n]))\n",
    "bad_df.sort_values(by='count', ascending=False, inplace=True)\n",
    "bad_df.loc[(bad_df['count']>3) & (bad_df['Database']!='CREA')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_verbs[clean_verbs['SemClass']=='aspectual']\n",
    "badverb = ['vaporize', 'extract', 'constitute', 'intensify', 'solidify', 'disperse', 'deepen', 'dampen', 'purify', \n",
    "          'omit', 'suffocate', 'subside', 'throb', 'resume', 'classify', 'submerge', 'suppress', 'consolidate', \n",
    "           'calm', 'involve']\n",
    "len(badverb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.arctanh(0.767)\n",
    "\n",
    "# from scipy.stats import zprob\n",
    "# def z_transform(r, n):\n",
    "#     z = np.log((1 + r) / (1 - r)) * (np.sqrt(n - 3) / 2)\n",
    "#     p = zprob(-z)\n",
    "#     return p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
